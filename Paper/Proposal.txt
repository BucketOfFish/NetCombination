Intro
-----

Suppose we have two fully independent neural networks of unspecified architecture. Furthermore, suppose both nets are continuously processing input data and subsequently affecting their environments via their outputs. A few examples of such nets are (1) a video game AI which plays through platformer levels, (2) a control system with a robotic arm and camera that attempts to pick up objects, (3) a living organism.

In this paper, we propose to investigate whether we can merge two such nets in a way that they become strongly coupled within a short time frame, while not degrading the performance of either net during the merging process. For example, if we had two independent robotic arm control systems each trained to recognize and pick up objects, we may wish to know how to merge the two systems into a single two-armed control system, where the ability of either arm to pick up objects individually is not significantly degraded during the merge process. If this is possible, it may become feasible to train and integrate neural nets in a modular fashion, so that old nets may receive upgrades without having to restart the entire training process with a combined architecture. For example, a human who wished to control a robotic limb may be able to merge their sensorimotor cortex with a limb that had pre-existing "muscle memory", rather than having to learn how to control the limb from scratch.

In order to investigate this question, we create a game similar to Pong, with a single difference - the ball teleports to a random y-location after each contact with a paddle. We split the visual input for this game into left and right, and train a separate reinforcement learning model for each half. One model sees the left half of the screen and can control the left paddle, and the other model does the same using the right side. Since paddles move slowly, the incoming y-location is random, and each model only gets to see one half of the screen, we expect the models will learn how to return serves, but will be too late to intercept many of them. However, if the models were combined into a single net that sees the whole screen and controls both paddles, we expect gameplay to improve significantly.

With the two half-screen models, we will investigate whether it is possible to create and train a single whole-screen model by adding additional connection neurons and linking them with existing neurons in the two half-screen nets. To avoid the trivial solution of simply retraining the entire combined net from scratch, we require the individual return rate of each paddle to not significantly decrease at any time during the merging. We wish to investigate how many new connecting neurons are required in order to merge two such nets of arbitrary size, and methods for quickly performing the merging. What happens if connecting neurons are forced to only create connections in localized areas of each net?

Misc Ideas
-----

Immediately after beginning the merging process, we want there to be no detrimental effect on the performance of either subnet. One way to enforce this is to simply set the initial weights of all connecting neurons to zero. That is, there are new nerons added to the net, and these neurons get weighted links to existing neurons in both nets. However, by initializing these weights to zero, communication across nets is removed. The input of net one has no effect on the output of net two, and vice versa.

When linking connecting neurons to existing nets, preferentiably connect inputs to neurons with many connections, and outputs to neurons with few connections. This way the connecting neuron can receive input from many areas of the net, and output to an area with little semantic competition.
